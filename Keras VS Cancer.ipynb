{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train patients: 798\n",
      "Valid patients: 797\n",
      "Create and compile model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:96: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((1, 1), input_shape=(1, 64, 64..., data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:98: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((1, 1), data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:99: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:100: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:102: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((1, 1), data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((1, 1), data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", data_format=\"channels_first\")`\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:106: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), data_format=\"channels_first\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 1560\n",
      "Number of valid files: 2044\n",
      "Fit model...\n",
      "Samples train: 10000, Samples valid: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:167: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\aliza\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:167: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., validation_data=<generator..., verbose=1, callbacks=[<keras.ca..., steps_per_epoch=10000, epochs=1, validation_steps=1000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 19417s 2s/step - loss: 1.9584 - acc: 0.8629 - val_loss: 6.3954 - val_acc: 0.6032\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'modal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-065696eeba09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_single_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m \u001b[0mmodal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"modal_Complete.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'modal' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is simple script with many limitation due to run on Kaggle CPU server.\n",
    "# There is used simple CNN with low number of conv layers and filters.\n",
    "# You can improve this script while run on local GPU just by changing some constants\n",
    "# It just shows the possible example of dataflow which can be used for solving this problem\n",
    "\n",
    "conf = dict()\n",
    "# Change this variable to 0 in case you want to use full dataset\n",
    "conf['use_sample_only'] = 1\n",
    "# Save weights\n",
    "conf['save_weights'] = 1\n",
    "# How many patients will be in train and validation set during training. Range: (0; 1)\n",
    "conf['train_valid_fraction'] = 0.5\n",
    "# Batch size for CNN [Depends on GPU and memory available]\n",
    "conf['batch_size'] = 200\n",
    "# Number of epochs for CNN training\n",
    "conf['nb_epoch'] = 1\n",
    "# Early stopping. Stop training after epochs without improving on validation\n",
    "conf['patience'] = 2\n",
    "# Shape of image for CNN (Larger the better, but you need to increase CNN as well)\n",
    "conf['image_shape'] = (64, 64)\n",
    "# Learning rate for CNN. Lower better accuracy, larger runtime.\n",
    "conf['learning_rate'] = 1e-2\n",
    "# Number of random samples to use during training per epoch \n",
    "conf['samples_train_per_epoch'] = 10000\n",
    "# Number of random samples to use during validation per epoch\n",
    "conf['samples_valid_per_epoch'] = 1000\n",
    "# Some variables to control CNN structure\n",
    "conf['level_1_filters'] = 4\n",
    "conf['level_2_filters'] = 8\n",
    "conf['dense_layer_size'] = 32\n",
    "conf['dropout_value'] = 0.5\n",
    "\n",
    "\n",
    "import dicom \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(2016)\n",
    "random.seed(2016)\n",
    "\n",
    "\n",
    "def load_and_normalize_dicom(path, x, y):\n",
    "    dicom1 = dicom.read_file(path)\n",
    "    dicom_img = dicom1.pixel_array.astype(np.float64)\n",
    "    mn = dicom_img.min()\n",
    "    mx = dicom_img.max()\n",
    "    if (mx - mn) != 0:\n",
    "        dicom_img = (dicom_img - mn)/(mx - mn)\n",
    "    else:\n",
    "        dicom_img[:, :] = 0\n",
    "    if dicom_img.shape != (x, y):\n",
    "        dicom_img = cv2.resize(dicom_img, (x, y), interpolation=cv2.INTER_CUBIC)\n",
    "    return dicom_img\n",
    "\n",
    "\n",
    "def batch_generator_train(files, train_csv_table, batch_size):\n",
    "    number_of_batches = np.ceil(len(files)/batch_size)\n",
    "    counter = 0\n",
    "    random.shuffle(files)\n",
    "    while True:\n",
    "        batch_files = files[batch_size*counter:batch_size*(counter+1)]\n",
    "        image_list = []\n",
    "        mask_list = []\n",
    "        for f in batch_files:\n",
    "            image = load_and_normalize_dicom(f, conf['image_shape'][0], conf['image_shape'][1])\n",
    "            patient_id = os.path.basename(os.path.dirname(f))\n",
    "            is_cancer = train_csv_table.loc[train_csv_table['id'] == patient_id]['cancer'].values[0]\n",
    "            if is_cancer == 0:\n",
    "                mask = [1, 0]\n",
    "            else:\n",
    "                mask = [0, 1]\n",
    "            image_list.append([image])\n",
    "            mask_list.append(mask)\n",
    "        counter += 1\n",
    "        image_list = np.array(image_list)\n",
    "        mask_list = np.array(mask_list)\n",
    "        # print(image_list.shape)\n",
    "        # print(mask_list.shape)\n",
    "        yield image_list, mask_list\n",
    "        if counter == number_of_batches:\n",
    "            random.shuffle(files)\n",
    "            counter = 0\n",
    "\n",
    "\n",
    "def get_custom_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(1, conf['image_shape'][0], conf['image_shape'][1]), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_1_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_1_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_2_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(conf['level_2_filters'], 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(conf['dense_layer_size'], activation='relu'))\n",
    "    model.add(Dropout(conf['dropout_value']))\n",
    "    model.add(Dense(conf['dense_layer_size'], activation='relu'))\n",
    "    model.add(Dropout(conf['dropout_value']))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=conf['learning_rate'], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_single_fold(train_data, fraction):\n",
    "    ids = train_data['id'].values\n",
    "    random.shuffle(ids)\n",
    "    split_point = int(round(fraction*len(ids)))\n",
    "    train_list = ids[:split_point]\n",
    "    valid_list = ids[split_point:]\n",
    "    return train_list, valid_list\n",
    "\n",
    "\n",
    "def create_single_model():\n",
    "\n",
    "    train_csv_table = pd.read_csv('D:/FYP/Data Sets/Labels/stage1_labels.csv')\n",
    "    train_patients, valid_patients = get_train_single_fold(train_csv_table, conf['train_valid_fraction'])\n",
    "    print('Train patients: {}'.format(len(train_patients)))\n",
    "    print('Valid patients: {}'.format(len(valid_patients)))\n",
    "\n",
    "    print('Create and compile model...')\n",
    "    model = get_custom_CNN()\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=conf['patience'], verbose=0),\n",
    "        # ModelCheckpoint('best.hdf5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]\n",
    "\n",
    "    get_dir = 'stage1'\n",
    "    if conf['use_sample_only'] == 1:\n",
    "        get_dir = 'sample_images'\n",
    "\n",
    "    train_files = []\n",
    "    for p in train_patients:\n",
    "        train_files += glob.glob(\"D:/FYP/Data Sets/{}/{}/*.dcm\".format(get_dir, p))\n",
    "    print('Number of train files: {}'.format(len(train_files)))\n",
    "\n",
    "    valid_files = []\n",
    "    for p in valid_patients:\n",
    "        valid_files += glob.glob(\"D:/FYP/Data Sets/{}/{}/*.dcm\".format(get_dir, p))\n",
    "    print('Number of valid files: {}'.format(len(valid_files)))\n",
    "\n",
    "    print('Fit model...')\n",
    "    print('Samples train: {}, Samples valid: {}'.format(conf['samples_train_per_epoch'], conf['samples_valid_per_epoch']))\n",
    "    fit = model.fit_generator(generator=batch_generator_train(train_files, train_csv_table, conf['batch_size']),\n",
    "                          nb_epoch=conf['nb_epoch'],\n",
    "                          samples_per_epoch=conf['samples_train_per_epoch'],\n",
    "                          validation_data=batch_generator_train(valid_files, train_csv_table, conf['batch_size']),\n",
    "                          nb_val_samples=conf['samples_valid_per_epoch'],\n",
    "                          verbose=1,\n",
    "                          callbacks=callbacks)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_submission(model):\n",
    "    sample_subm = pd.read_csv(\"../input/stage1_sample_submission.csv\")\n",
    "    ids = sample_subm['id'].values\n",
    "    for id in ids:\n",
    "        print('Predict for patient {}'.format(id))\n",
    "        files = glob.glob(\"D:/FYP/Data Sets/stage1/{}/*.dcm\".format(id))\n",
    "        image_list = []\n",
    "        for f in files:\n",
    "            image = load_and_normalize_dicom(f, conf['image_shape'][0], conf['image_shape'][1])\n",
    "            image_list.append([image])\n",
    "        image_list = np.array(image_list)\n",
    "        batch_size = len(image_list)\n",
    "        predictions = model.predict(image_list, verbose=1, batch_size=batch_size)\n",
    "        pred_value = predictions[:, 1].mean()\n",
    "        sample_subm.loc[sample_subm['id'] == id, 'cancer'] = pred_value\n",
    "    sample_subm.to_csv(\"subm.csv\", index=False)\n",
    "\n",
    "model = create_single_model()\n",
    "modal.save(\"modal_Complete.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
